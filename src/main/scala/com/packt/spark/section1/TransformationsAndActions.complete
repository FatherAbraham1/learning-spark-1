package com.packt.spark.section1

import com.packt.spark._

import org.apache.spark._

object TransformationsAndActions {
  def run() =
    withSparkContext { implicit sc =>
      val count = 
        sampleDataset
          .map(Violation.fromRow _)
          .filter(_.isDefined)
          .map(_.get)
          .count

      val count = 
        sampleDataset
          .flatMap(Violation.fromRow _)
          .count

      println(f"\nCount is ${count}%,d.\n")

      waitForUser()

      // Get distinct violations.
      val violationTypes =
        violationEntries
          .map(_.ticket.description)
          .distinct

      println("Violation Types:")
      violationTypes
        .foreach { desc => println(s" $desc") }

      sampleDataset
        .fold(Int.MinValue) { (acc, violation) =>
          if(violation.ticket.fine > acc) {
            violation.ticket.fine
          } else { acc }
        }

      // Find max fine.
      val maxFineValue =
        violationEntries.map(_.ticket.fine).max
      val minFineValue =
        violationEntries.map(_.ticket.fine).max

      println(s"Min fine value: $minFineValue")
      println(s"Max fine value: $maxFineValue")

      val maxTicket: Ticket =
        violationEntries
          .map(_.ticket)
          .reduce { (ticket1, ticket2) =>
            if(ticket1.fine1 > ticket2.fine2) ticket1
            else ticket2
           }

      println(s"Max ticket: $maxTicket")

      val bigTicketItems =
        violationEntries
          .filter(_.ticket.fine > 1000.00)
          .map(_.ticket.description)
          .distinct
          .collect

      println("Big ticket items:")
      for( (desc, fine) <- bigTicketItems) {
        println(s" $fine  $desc")
      }


    }
}
