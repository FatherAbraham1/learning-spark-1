package com.packt.spark.section2

import com.packt.spark._
import org.apache.spark._

import geotrellis.vector._

object AnatomyOfAnRDD extends ExampleApp {
  def run() =
    withSparkContext { implicit sc =>
      val totalExtent =
        fullDataset
          .flatMap(Violation.fromRow _)
          .map(_.location)
          .aggregate(None: Option[Extent])({ 
            (extent, point) =>
              extent match {
                case Some(e) =>
                  Some(e.expandToInclude(point))
                case None =>
                  Some(Extent(point.x, point.y, point.x, point.y))
              }
          }, { 
            (extent1, extent2) =>
              extent1 match {
                case Some(e1) =>
                  extent2 match {
                    case Some(e2) => Some(e1.combine(e2))
                    case None => Some(e1)
                  }
                case None => extent2
              }
          })

      println(s"Total extent: $totalExtent")
      waitForUser()
    }
}
