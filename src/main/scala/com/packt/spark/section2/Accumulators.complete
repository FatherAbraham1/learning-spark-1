package com.packt.spark.section2

import com.packt.spark._
import org.apache.spark._

object Accumulators extends ExampleApp {
  def run() =
    withSparkContext { implicit sc =>
      val infoAcc = sc.accumulable(DatasetInfo())

      val violationEntries = 
        sampleDataset
          .flatMap { line =>
            val parsed = Violation.fromRow(line)
            infoAcc += parsed
            parsed
           }

      // OR

      val violationEntries = 
        sampleDataset
          .map(Violation.fromRow _)
          .map(infoAcc += _)
          .flatten

      // More computations would happen here...
      violationEntries.foreach { x =>  }

      val DatasetInfo(
        dateRange,
        validCount,
        invalidCount,
        bigTicketItems,
        totalFines
      ) = infoAcc.value

      println(s"Valid count: $validCount")
      println(s"Invalid count: $invalidCount")
      println(s"Date range: ${dateRange.start} to ${dateRange.end}")
      println("Big ticket items:")
      for( (desc, fine) <- bigTicketItems) {
        println(s" $fine  $desc")
      }

    }
}
