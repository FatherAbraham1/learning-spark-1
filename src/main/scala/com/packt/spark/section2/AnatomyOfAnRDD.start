package com.packt.spark.section2

import com.packt.spark._
import org.apache.spark._

import geotrellis.vector._
import geotrellis.vector.io.json._

object AnatomyOfAnRDD extends ExampleApp {
  def run() =
    withSparkContext { implicit sc =>
      val violations: RDD[Violation] =
        fullDataset
          .flatMap(Violation.fromRow _)
          .filter(_.ticket.fine > 5.0)

      val count = violations.count
      println(s"Count is $count")
      waitForUser()
    }
}
